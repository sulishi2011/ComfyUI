#!/usr/bin/env python3
"""
æµ‹è¯•æ¨¡å‹ç¼“å­˜åŠŸèƒ½
éªŒè¯åŒä¸€æ¨¡å‹åªåŠ è½½ä¸€æ¬¡åˆ° CPU RAM
"""

import os
import sys
import time
import psutil
import logging

# æ·»åŠ  ComfyUI è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import comfy.utils

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def get_memory_usage_gb():
    """è·å–å½“å‰è¿›ç¨‹çš„å†…å­˜å ç”¨ï¼ˆGBï¼‰"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024**3

def test_cache():
    """æµ‹è¯•ç¼“å­˜åŠŸèƒ½"""

    print("=" * 60)
    print("æµ‹è¯•æ¨¡å‹ç¼“å­˜åŠŸèƒ½")
    print("=" * 60)

    # æŸ¥æ‰¾ä¸€ä¸ªæµ‹è¯•æ¨¡å‹æ–‡ä»¶
    model_dirs = [
        "models/checkpoints",
        "models/unet",
        "models/clip",
    ]

    test_model = None
    for model_dir in model_dirs:
        if os.path.exists(model_dir):
            files = [f for f in os.listdir(model_dir) if f.endswith(('.safetensors', '.ckpt', '.pt'))]
            if files:
                test_model = os.path.join(model_dir, files[0])
                break

    if not test_model:
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ°æµ‹è¯•æ¨¡å‹æ–‡ä»¶")
        print("è¯·ç¡®ä¿ models/checkpoints æˆ– models/unet ç›®å½•ä¸­æœ‰æ¨¡å‹æ–‡ä»¶")
        return

    print(f"\nğŸ“ æµ‹è¯•æ¨¡å‹: {test_model}")
    file_size = os.path.getsize(test_model) / 1024**3
    print(f"ğŸ“¦ æ–‡ä»¶å¤§å°: {file_size:.2f} GB")
    print()

    # æµ‹è¯• 1: é¦–æ¬¡åŠ è½½
    print("=" * 60)
    print("æµ‹è¯• 1: é¦–æ¬¡åŠ è½½æ¨¡å‹")
    print("=" * 60)

    mem_before = get_memory_usage_gb()
    print(f"åŠ è½½å‰å†…å­˜: {mem_before:.2f} GB")

    start_time = time.time()
    sd1 = comfy.utils.load_torch_file_cached(test_model)
    load_time_1 = time.time() - start_time

    mem_after_1 = get_memory_usage_gb()
    mem_increase_1 = mem_after_1 - mem_before

    print(f"åŠ è½½åå†…å­˜: {mem_after_1:.2f} GB")
    print(f"å†…å­˜å¢åŠ : {mem_increase_1:.2f} GB")
    print(f"åŠ è½½æ—¶é—´: {load_time_1:.2f} ç§’")
    print(f"çŠ¶æ€: âœ… é¦–æ¬¡åŠ è½½å®Œæˆï¼ˆé¢„æœŸä»ç£ç›˜è¯»å–ï¼‰")
    print()

    # æµ‹è¯• 2: ç¬¬äºŒæ¬¡åŠ è½½ï¼ˆåº”è¯¥ä½¿ç”¨ç¼“å­˜ï¼‰
    print("=" * 60)
    print("æµ‹è¯• 2: ç¬¬äºŒæ¬¡åŠ è½½åŒä¸€æ¨¡å‹ï¼ˆåº”è¯¥ä½¿ç”¨ç¼“å­˜ï¼‰")
    print("=" * 60)

    mem_before_2 = get_memory_usage_gb()
    print(f"åŠ è½½å‰å†…å­˜: {mem_before_2:.2f} GB")

    start_time = time.time()
    sd2 = comfy.utils.load_torch_file_cached(test_model)
    load_time_2 = time.time() - start_time

    mem_after_2 = get_memory_usage_gb()
    mem_increase_2 = mem_after_2 - mem_before_2

    print(f"åŠ è½½åå†…å­˜: {mem_after_2:.2f} GB")
    print(f"å†…å­˜å¢åŠ : {mem_increase_2:.2f} GB")
    print(f"åŠ è½½æ—¶é—´: {load_time_2:.2f} ç§’")

    # éªŒè¯æ˜¯å¦ä½¿ç”¨äº†ç¼“å­˜
    if sd1 is sd2:
        print(f"çŠ¶æ€: âœ… ä½¿ç”¨ç¼“å­˜æˆåŠŸï¼ˆä¸¤æ¬¡åŠ è½½è¿”å›åŒä¸€å¯¹è±¡ï¼‰")
    else:
        print(f"çŠ¶æ€: âš ï¸  è­¦å‘Šï¼šä¸¤æ¬¡åŠ è½½è¿”å›ä¸åŒå¯¹è±¡")
    print()

    # æµ‹è¯• 3: ç¬¬ä¸‰æ¬¡åŠ è½½
    print("=" * 60)
    print("æµ‹è¯• 3: ç¬¬ä¸‰æ¬¡åŠ è½½ï¼ˆæ¨¡æ‹Ÿå¤š GPUï¼‰")
    print("=" * 60)

    start_time = time.time()
    sd3 = comfy.utils.load_torch_file_cached(test_model)
    load_time_3 = time.time() - start_time

    mem_after_3 = get_memory_usage_gb()
    mem_increase_3 = mem_after_3 - mem_before_2

    print(f"åŠ è½½åå†…å­˜: {mem_after_3:.2f} GB")
    print(f"æ€»å†…å­˜å¢åŠ : {mem_increase_3:.2f} GB")
    print(f"åŠ è½½æ—¶é—´: {load_time_3:.2f} ç§’")
    print()

    # æµ‹è¯• 4: ç¬¬å››æ¬¡åŠ è½½
    print("=" * 60)
    print("æµ‹è¯• 4: ç¬¬å››æ¬¡åŠ è½½ï¼ˆæ¨¡æ‹Ÿ 4 GPUï¼‰")
    print("=" * 60)

    start_time = time.time()
    sd4 = comfy.utils.load_torch_file_cached(test_model)
    load_time_4 = time.time() - start_time

    mem_after_4 = get_memory_usage_gb()
    mem_total_increase = mem_after_4 - mem_before

    print(f"åŠ è½½åå†…å­˜: {mem_after_4:.2f} GB")
    print(f"æ€»å†…å­˜å¢åŠ : {mem_total_increase:.2f} GB")
    print(f"åŠ è½½æ—¶é—´: {load_time_4:.2f} ç§’")
    print()

    # æ€»ç»“
    print("=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    print(f"æ¨¡å‹æ–‡ä»¶å¤§å°: {file_size:.2f} GB")
    print(f"CPU RAM æ€»å¢åŠ : {mem_total_increase:.2f} GB")
    print()
    print(f"é¦–æ¬¡åŠ è½½æ—¶é—´: {load_time_1:.2f} ç§’ï¼ˆç£ç›˜è¯»å–ï¼‰")
    print(f"ç¬¬äºŒæ¬¡åŠ è½½æ—¶é—´: {load_time_2:.4f} ç§’ï¼ˆç¼“å­˜ï¼‰")
    print(f"ç¬¬ä¸‰æ¬¡åŠ è½½æ—¶é—´: {load_time_3:.4f} ç§’ï¼ˆç¼“å­˜ï¼‰")
    print(f"ç¬¬å››æ¬¡åŠ è½½æ—¶é—´: {load_time_4:.4f} ç§’ï¼ˆç¼“å­˜ï¼‰")
    print()

    if load_time_2 < load_time_1 / 10:  # ç¼“å­˜åº”è¯¥å¿«è‡³å°‘ 10 å€
        print(f"âœ… é€Ÿåº¦æå‡: {load_time_1/load_time_2:.0f}x")
    else:
        print(f"âš ï¸  è­¦å‘Š: ç¼“å­˜é€Ÿåº¦æå‡ä¸æ˜æ˜¾")

    print()
    expected_mem = file_size * 1.2  # è€ƒè™‘ä¸€äº›å¼€é”€
    if mem_total_increase < expected_mem * 1.5:
        print(f"âœ… å†…å­˜æ•ˆç‡: 4 æ¬¡åŠ è½½åªå ç”¨çº¦ 1 ä»½æ¨¡å‹å¤§å°çš„å†…å­˜")
        print(f"   ç†è®º: {file_size:.2f} GB Ã— 4 = {file_size * 4:.2f} GB")
        print(f"   å®é™…: {mem_total_increase:.2f} GB")
        print(f"   èŠ‚çœ: {(1 - mem_total_increase / (file_size * 4)) * 100:.1f}%")
    else:
        print(f"âš ï¸  è­¦å‘Š: å†…å­˜å ç”¨è¶…å‡ºé¢„æœŸ")

    print()

    # éªŒè¯ç¼“å­˜çŠ¶æ€
    if hasattr(comfy.utils, '_state_dict_cache'):
        cache_size = len(comfy.utils._state_dict_cache)
        print(f"ğŸ“Š ç¼“å­˜çŠ¶æ€:")
        print(f"   ç¼“å­˜æ¡ç›®æ•°: {cache_size}")
        print(f"   ç¼“å­˜çš„æ–‡ä»¶: {list(comfy.utils._state_dict_cache.keys())}")

    print()
    print("=" * 60)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)

if __name__ == "__main__":
    try:
        test_cache()
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
