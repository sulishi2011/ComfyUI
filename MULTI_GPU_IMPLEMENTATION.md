# ComfyUI å¤š GPU å¹¶è¡Œæ”¹é€ æ–¹æ¡ˆ - æ‰§è¡Œæ–‡æ¡£ v3.5

> **ä½œè€…**: Claude (Anthropic)
> **ç‰ˆæœ¬**: 3.5 ç»ˆæç‰ˆ
> **æ—¥æœŸ**: 2025-11-05
> **ç›®æ ‡**: å•è¿›ç¨‹ + å…±äº« RAM + 4 GPU çœŸå¹¶è¡Œ + é›¶æ”¹ workflow

---

## ğŸ“‹ ç›®å½•

1. [æ–¹æ¡ˆæ¦‚è¿°](#1-æ–¹æ¡ˆæ¦‚è¿°)
2. [ç¯å¢ƒè¦æ±‚](#2-ç¯å¢ƒè¦æ±‚)
3. [æ”¹åŠ¨æ¸…å•](#3-æ”¹åŠ¨æ¸…å•)
4. [å®æ–½æ­¥éª¤](#4-å®æ–½æ­¥éª¤)
   - [Step 1: å¤‡ä»½ä¸å‡†å¤‡](#step-1-å¤‡ä»½ä¸å‡†å¤‡)
   - [Step 2: ä¿®æ”¹ model_management.py](#step-2-ä¿®æ”¹-model_managementpy)
   - [Step 3: ä¿®æ”¹ main.py](#step-3-ä¿®æ”¹-mainpy)
   - [Step 4: ä¿®æ”¹ server.py](#step-4-ä¿®æ”¹-serverpy)
   - [Step 5: é…ç½® Nginx](#step-5-é…ç½®-nginx)
   - [Step 6: æµ‹è¯•éªŒè¯](#step-6-æµ‹è¯•éªŒè¯)
5. [éªŒæ”¶æ ‡å‡†](#5-éªŒæ”¶æ ‡å‡†)
6. [å›æ»šæ–¹æ¡ˆ](#6-å›æ»šæ–¹æ¡ˆ)
7. [æ•…éšœæ’æŸ¥](#7-æ•…éšœæ’æŸ¥)
8. [æ€§èƒ½è°ƒä¼˜](#8-æ€§èƒ½è°ƒä¼˜)

---

## 1. æ–¹æ¡ˆæ¦‚è¿°

### 1.1 æ ¸å¿ƒç›®æ ‡

| ç›®æ ‡ | å®ç°æ–¹å¼ |
|------|---------|
| **å…±äº« RAM** | å•è¿›ç¨‹è¿è¡Œï¼Œæ¨¡å‹æƒé‡åœ¨ RAM ä¸­åªå­˜ä¸€ä»½ |
| **4 GPU å¹¶è¡Œ** | 4 ä¸ªé˜Ÿåˆ— + 4 ä¸ª worker çº¿ç¨‹ï¼Œå„è‡ªç»‘å®šä¸€å¼  GPU |
| **å›ºå®šè·¯ç”±** | æ¯ä¸ªå…¥å£ï¼ˆç«¯å£/è·¯å¾„ï¼‰å›ºå®šæ˜ å°„åˆ°ä¸€å¼  GPU |
| **é›¶æ”¹ workflow** | ä¸ä¿®æ”¹ç°æœ‰æ¨¡æ¿ï¼Œå®Œå…¨å…¼å®¹ |
| **å¯å›æ»š** | ç¯å¢ƒå˜é‡å¼€å…³ï¼Œéšæ—¶æ¢å¤åŸæœ‰å• GPU æ¨¡å¼ |

### 1.2 æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Nginx/Caddy åå‘ä»£ç†                        â”‚
â”‚  :8181 (GPU 0) :8182 (GPU 1) :8183 (GPU 2) :8184 (GPU 3)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ X-TARGET-GPU: 0         â”‚            â”‚
             â”‚            â”‚ X-TARGET-GPU: 1         â”‚
             â”‚            â”‚            â”‚ X-TARGET-GPU: 2
             â”‚            â”‚            â”‚            â”‚ X-TARGET-GPU: 3
             â–¼            â–¼            â–¼            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         ComfyUI å•è¿›ç¨‹ (:8188)                       â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚  /prompt handler (è·¯ç”±åˆ†å‘)                   â”‚   â”‚
    â”‚  â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚     â”‚          â”‚          â”‚          â”‚             â”‚
    â”‚  â”Œâ”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â–¼â”€â”€â”€â”         â”‚
    â”‚  â”‚Queue0â”‚  â”‚Queue1â”‚  â”‚Queue2â”‚  â”‚Queue3â”‚         â”‚
    â”‚  â””â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”˜         â”‚
    â”‚     â”‚          â”‚          â”‚          â”‚             â”‚
    â”‚  â”Œâ”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”        â”‚
    â”‚  â”‚Worker0â”‚ â”‚Worker1â”‚ â”‚Worker2â”‚ â”‚Worker3â”‚        â”‚
    â”‚  â”‚ GPU:0 â”‚ â”‚ GPU:1 â”‚ â”‚ GPU:2 â”‚ â”‚ GPU:3 â”‚        â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
    â”‚                                                    â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚  æ¨¡å‹ç¼“å­˜ï¼ˆæŒ‰è®¾å¤‡åˆ†åŒºï¼‰                     â”‚    â”‚
    â”‚  â”‚  device_cache[0] â†’ [models on GPU 0]     â”‚    â”‚
    â”‚  â”‚  device_cache[1] â†’ [models on GPU 1]     â”‚    â”‚
    â”‚  â”‚  device_cache[2] â†’ [models on GPU 2]     â”‚    â”‚
    â”‚  â”‚  device_cache[3] â†’ [models on GPU 3]     â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚              â”‚              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   GPU 0      â”‚ â”‚   GPU 1     â”‚ â”‚   GPU 2/3   â”‚
    â”‚  VRAM 24GB   â”‚ â”‚  VRAM 24GB  â”‚ â”‚  VRAM 24GB  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â–²              â–²              â–²
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              å…±äº« RAM ä¸­çš„æ¨¡å‹æƒé‡
              (åªåŠ è½½ä¸€æ¬¡ï¼Œ4 GPU å…±äº«)
```

### 1.3 æ ¸å¿ƒç‰¹æ€§

- âœ… **ç‰¹æ€§å¼€å…³**: `COMFY_MULTI_GPU_SCHED=1` å¯ç”¨ï¼Œ`=0` å›é€€åŸæ¨¡å¼
- âœ… **è®¾å¤‡åˆ†åŒºç¼“å­˜**: é¿å…å¤šçº¿ç¨‹ç«äº‰ï¼Œæ”¯æŒçœŸå¹¶è¡Œ
- âœ… **è§‚æµ‹æŒ‡æ ‡**: æ¯ GPU çš„é˜Ÿåˆ—é•¿åº¦ã€ç­‰å¾…æ—¶é—´ã€æ‰§è¡Œæ—¶é—´ã€OOM æ¬¡æ•°
- âœ… **GPU é¢„çƒ­**: å¯åŠ¨æ—¶é¢„åˆ†é…æ˜¾å­˜ï¼Œé™ä½é¦–æ¬¡è¯·æ±‚å»¶è¿Ÿ
- âœ… **OOM å¼¹æ€§**: è‡ªåŠ¨æ¸…ç¼“å­˜é‡è¯•
- âœ… **å…¼å®¹æ€§æ£€æŸ¥**: å¯åŠ¨æ—¶æ‰«æ custom_nodes ä¸­çš„è®¾å¤‡ç¡¬ç¼–ç 

---

## 2. ç¯å¢ƒè¦æ±‚

### 2.1 ç¡¬ä»¶è¦æ±‚

| ç»„ä»¶ | è¦æ±‚ |
|------|------|
| **GPU** | 4 å¼  NVIDIA GPUï¼ˆæ¨èåŒå‹å·ï¼Œå¦‚ 4x RTX 4090ï¼‰ |
| **VRAM** | æ¯å¡è‡³å°‘ 16GBï¼ˆæ¨è 24GB+ï¼‰ |
| **RAM** | è‡³å°‘ 64GBï¼ˆæ¨è 128GB+ï¼Œå–å†³äºæ¨¡å‹å¤§å°ï¼‰ |
| **å­˜å‚¨** | SSDï¼ˆæ¨¡å‹æ–‡ä»¶è¯»å–æ€§èƒ½å…³é”®ï¼‰ |

### 2.2 è½¯ä»¶è¦æ±‚

| ç»„ä»¶ | ç‰ˆæœ¬ |
|------|------|
| **Python** | 3.10+ |
| **PyTorch** | 2.0+ with CUDA |
| **CUDA** | 11.8+ |
| **ComfyUI** | å½“å‰ç‰ˆæœ¬ï¼ˆå·²æµ‹è¯• v0.3.68ï¼‰ |
| **Nginx/Caddy** | ä»»æ„ç‰ˆæœ¬ |

### 2.3 æ£€æŸ¥å‘½ä»¤

```bash
# æ£€æŸ¥ GPU æ•°é‡
nvidia-smi --list-gpus

# æ£€æŸ¥ CUDA
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPUs: {torch.cuda.device_count()}')"

# æ£€æŸ¥ ComfyUI ç‰ˆæœ¬
python main.py --version
```

---

## 3. æ”¹åŠ¨æ¸…å•

### 3.1 æ–‡ä»¶æ”¹åŠ¨æ¦‚è§ˆ

| æ–‡ä»¶ | æ”¹åŠ¨ç±»å‹ | è¡Œæ•° | é£é™©ç­‰çº§ |
|------|---------|------|---------|
| `comfy/model_management.py` | ä¿®æ”¹ | +80 | ä¸­ |
| `main.py` | ä¿®æ”¹ | +120 | ä¸­ |
| `server.py` | ä¿®æ”¹ | +15 | ä½ |
| `nginx.conf` (æ–°å¢) | æ–°å»º | +40 | ä½ |
| `start_multi_gpu.sh` (æ–°å¢) | æ–°å»º | +10 | ä½ |

**æ€»è®¡**: ~265 è¡Œæ–°å¢ä»£ç 

### 3.2 ä¸æ”¹åŠ¨çš„å†…å®¹

- âŒ ä¸æ”¹ workflow æ¨¡æ¿
- âŒ ä¸æ”¹è‡ªå®šä¹‰èŠ‚ç‚¹ï¼ˆé™¤éæœ‰ç¡¬ç¼–ç é—®é¢˜ï¼‰
- âŒ ä¸æ”¹ execution.py æ ¸å¿ƒæ‰§è¡Œé€»è¾‘
- âŒ ä¸æ”¹å‰ç«¯ä»£ç 

---

## 4. å®æ–½æ­¥éª¤

### Step 1: å¤‡ä»½ä¸å‡†å¤‡

#### 1.1 å¤‡ä»½åŸå§‹æ–‡ä»¶

```bash
cd /path/to/ComfyUI

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p backups/$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="backups/$(date +%Y%m%d_%H%M%S)"

# å¤‡ä»½å…³é”®æ–‡ä»¶
cp comfy/model_management.py "$BACKUP_DIR/"
cp main.py "$BACKUP_DIR/"
cp server.py "$BACKUP_DIR/"

echo "Backup completed: $BACKUP_DIR"
```

#### 1.2 åˆ›å»º Git åˆ†æ”¯ï¼ˆå¦‚æœä½¿ç”¨ Gitï¼‰

```bash
git checkout -b feature/multi-gpu-sched
git status
```

#### 1.3 è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆæµ‹è¯•ï¼‰

```bash
# å…ˆä¸å¯ç”¨ï¼Œç¡®ä¿åŸæ¨¡å¼æ­£å¸¸
export COMFY_MULTI_GPU_SCHED=0
export COMFY_NUM_GPUS=4

# æµ‹è¯•åŸæœ‰åŠŸèƒ½
python main.py --listen 0.0.0.0 --port 8188
# Ctrl+C åœæ­¢
```

---

### Step 2: ä¿®æ”¹ model_management.py

#### 2.1 ç›®æ ‡

- æ·»åŠ æŒ‰è®¾å¤‡åˆ†åŒºçš„æ¨¡å‹ç¼“å­˜
- ä¿®æ”¹ `free_memory()` æ”¯æŒè®¾å¤‡éš”ç¦»
- ä¿®æ”¹ `load_models_gpu()` ä½¿ç”¨è®¾å¤‡ç¼“å­˜
- ä¿®æ”¹ `soft_empty_cache()` æ”¯æŒæŒ‰è®¾å¤‡æ¸…ç†

#### 2.2 å…·ä½“æ”¹åŠ¨

**ä½ç½®**: `comfy/model_management.py`

**åœ¨æ–‡ä»¶å¼€å¤´ï¼ˆçº¦ line 30 é™„è¿‘ï¼‰æ·»åŠ **:

```python
import os
import threading

# ============ å¤š GPU è°ƒåº¦ç›¸å…³é…ç½® ============
ENABLE_MULTI_GPU = os.getenv('COMFY_MULTI_GPU_SCHED', '0') == '1'

if ENABLE_MULTI_GPU:
    _current_loaded_models_by_device = {}  # device_id -> [LoadedModel]
    _model_cache_lock = threading.RLock()
    _use_device_cache = True
    logging.info("âœ… Multi-GPU scheduling ENABLED")
else:
    _use_device_cache = False
    logging.info("â„¹ï¸  Multi-GPU scheduling DISABLED (using default mode)")
```

**åœ¨ `current_loaded_models = []` å®šä¹‰åï¼ˆçº¦ line 449ï¼‰æ·»åŠ **:

```python
# åŸæœ‰ä»£ç ä¿æŒä¸å˜
current_loaded_models = []

# æ–°å¢ï¼šç»Ÿä¸€ç¼“å­˜è®¿é—®å…¥å£
def _get_current_loaded_models(device=None):
    """
    ç»Ÿä¸€ç¼“å­˜è®¿é—®å…¥å£ï¼šæ ¹æ®å¼€å…³è‡ªåŠ¨è¿”å›æ­£ç¡®çš„ç¼“å­˜

    Args:
        device: torch.device å¯¹è±¡ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨å½“å‰è®¾å¤‡

    Returns:
        list: å¯¹åº”è®¾å¤‡çš„ LoadedModel åˆ—è¡¨
    """
    if _use_device_cache:
        if device is None:
            device = get_torch_device()

        # æå–è®¾å¤‡ ID
        if hasattr(device, 'index') and device.index is not None:
            device_id = device.index
        elif hasattr(device, 'type') and device.type == 'cuda':
            device_id = torch.cuda.current_device()
        else:
            device_id = 0  # CPU æˆ–å…¶ä»–è®¾å¤‡ç»Ÿä¸€ç”¨ 0

        # æŒ‰è®¾å¤‡åˆ†åŒº
        with _model_cache_lock:
            if device_id not in _current_loaded_models_by_device:
                _current_loaded_models_by_device[device_id] = []
                logging.debug(f"Created model cache for device {device_id}")
            return _current_loaded_models_by_device[device_id]
    else:
        # å…¼å®¹åŸæœ‰æ¨¡å¼
        global current_loaded_models
        return current_loaded_models
```

**ä¿®æ”¹ `free_memory()` å‡½æ•°ï¼ˆçº¦ line 580ï¼‰**:

æ‰¾åˆ°å‡½æ•°å®šä¹‰ï¼š
```python
def free_memory(memory_required, device, keep_loaded=[]):
```

åœ¨å‡½æ•°å¼€å¤´æ·»åŠ ï¼š
```python
def free_memory(memory_required, device, keep_loaded=[]):
    cleanup_models_gc()
    unloaded_model = []
    can_unload = []
    unloaded_models = []

    # ========== æ–°å¢ï¼šä½¿ç”¨è®¾å¤‡ä¸“å±ç¼“å­˜ ==========
    current_loaded = _get_current_loaded_models(device)
    # =========================================

    # åŸæœ‰é€»è¾‘ç»§ç»­ï¼Œä½†ä½¿ç”¨ current_loaded ä»£æ›¿ current_loaded_models
    for i in range(len(current_loaded) - 1, -1, -1):
        shift_model = current_loaded[i]
        # ... åç»­é€»è¾‘ä¸å˜
```

**æ³¨æ„**: å°†å‡½æ•°å†…æ‰€æœ‰çš„ `current_loaded_models` æ›¿æ¢ä¸º `current_loaded`

**ä¿®æ”¹ `load_models_gpu()` å‡½æ•°ï¼ˆçº¦ line 617ï¼‰**:

åœ¨å‡½æ•°ä¸­æ‰¾åˆ°ä½¿ç”¨ `current_loaded_models` çš„åœ°æ–¹ï¼Œæ”¹ä¸ºä½¿ç”¨è®¾å¤‡ç¼“å­˜ï¼š

```python
def load_models_gpu(models, memory_required=0, force_patch_weights=False, minimum_memory_required=None, force_full_load=False):
    # ... å‰é¢é€»è¾‘ä¸å˜ï¼Œç›´åˆ° models_to_load = [] å

    for loaded_model in models_to_load:
        # ========== æ–°å¢ï¼šè·å–è®¾å¤‡ä¸“å±ç¼“å­˜ ==========
        device = loaded_model.device
        current_loaded = _get_current_loaded_models(device)
        # =========================================

        # æ£€æŸ¥æ˜¯å¦å·²åœ¨å½“å‰è®¾å¤‡ç¼“å­˜ä¸­
        try:
            loaded_model_index = current_loaded.index(loaded_model)
        except:
            loaded_model_index = None

        # ... åç»­é€»è¾‘ä½¿ç”¨ current_loaded ä»£æ›¿ current_loaded_models
```

**ä¿®æ”¹ `soft_empty_cache()` å‡½æ•°ï¼ˆçº¦ line 1445ï¼‰**:

```python
def soft_empty_cache(device=None):
    """
    æ¸…ç©º CUDA ç¼“å­˜

    Args:
        device: æŒ‡å®šè®¾å¤‡ï¼Œå¦‚æœä¸º None åˆ™æ¸…ç©ºæ‰€æœ‰è®¾å¤‡
    """
    global cpu_state

    # æ–°å¢ï¼šæ”¯æŒæŒ‰è®¾å¤‡æ¸…ç†
    if device is not None and hasattr(device, 'type'):
        if device.type == 'cuda':
            device_id = device.index if hasattr(device, 'index') else 0
            with torch.cuda.device(device):
                torch.cuda.empty_cache()
                torch.cuda.ipc_collect()
            logging.debug(f"Cleared cache for device cuda:{device_id}")
            return
        elif device.type == 'xpu':
            torch.xpu.empty_cache()
            return
        elif device.type == 'npu':
            torch.npu.empty_cache()
            return
        elif device.type == 'mlu':
            torch.mlu.empty_cache()
            return

    # åŸæœ‰é€»è¾‘ï¼šæ¸…ç©ºæ‰€æœ‰è®¾å¤‡
    if cpu_state == CPUState.MPS:
        torch.mps.empty_cache()
    elif is_intel_xpu():
        torch.xpu.empty_cache()
    elif is_ascend_npu():
        torch.npu.empty_cache()
    elif is_mlu():
        torch.mlu.empty_cache()
    elif torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
```

**ä¿®æ”¹ `loaded_models()` å‡½æ•°ï¼ˆå¦‚æœæœ‰ï¼‰**:

```python
def loaded_models(only_currently_used=False, device=None):
    """
    è·å–å·²åŠ è½½çš„æ¨¡å‹åˆ—è¡¨

    Args:
        only_currently_used: æ˜¯å¦åªè¿”å›æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹
        device: æŒ‡å®šè®¾å¤‡ï¼Œå¦‚æœä¸º None åˆ™è¿”å›æ‰€æœ‰è®¾å¤‡çš„æ¨¡å‹
    """
    if device is not None:
        # è¿”å›æŒ‡å®šè®¾å¤‡çš„æ¨¡å‹
        current_loaded = _get_current_loaded_models(device)
        if only_currently_used:
            return [x for x in current_loaded if x.currently_used]
        return current_loaded
    else:
        # è¿”å›æ‰€æœ‰è®¾å¤‡çš„æ¨¡å‹
        if _use_device_cache:
            all_models = []
            with _model_cache_lock:
                for cache in _current_loaded_models_by_device.values():
                    all_models.extend(cache)
            if only_currently_used:
                return [x for x in all_models if x.currently_used]
            return all_models
        else:
            # åŸæœ‰æ¨¡å¼
            if only_currently_used:
                return [x for x in current_loaded_models if x.currently_used]
            return current_loaded_models
```

#### 2.3 éªŒè¯æ”¹åŠ¨

```bash
# è¯­æ³•æ£€æŸ¥
python -m py_compile comfy/model_management.py

# å¦‚æœæœ‰é”™è¯¯ä¼šæŠ¥å‡ºæ¥
echo $?  # åº”è¯¥è¿”å› 0
```

---

### Step 3: ä¿®æ”¹ main.py

#### 3.1 ç›®æ ‡

- æ·»åŠ å¤š GPU worker å‡½æ•°
- æ·»åŠ è§‚æµ‹æŒ‡æ ‡ç±»
- æ·»åŠ  GPU é¢„çƒ­å‡½æ•°
- æ·»åŠ å…¼å®¹æ€§æ£€æŸ¥å‡½æ•°
- ä¿®æ”¹ `start_comfyui()` æ”¯æŒå¤šé˜Ÿåˆ—å¯åŠ¨

#### 3.2 å…·ä½“æ”¹åŠ¨

**ä½ç½®**: `main.py`

**åœ¨æ–‡ä»¶å¼€å¤´ï¼ˆimport éƒ¨åˆ†åï¼‰æ·»åŠ **:

```python
import os
import gc
import time
from collections import deque

# ============ å¤š GPU è°ƒåº¦é…ç½® ============
ENABLE_MULTI_GPU = os.getenv('COMFY_MULTI_GPU_SCHED', '0') == '1'
NUM_GPUS = int(os.getenv('COMFY_NUM_GPUS', '4')) if ENABLE_MULTI_GPU else 1

if ENABLE_MULTI_GPU:
    logging.info(f"ğŸš€ Multi-GPU mode ENABLED with {NUM_GPUS} GPUs")
else:
    logging.info("â„¹ï¸  Single-GPU mode (default)")
```

**åœ¨ `prompt_worker()` å‡½æ•°åæ·»åŠ æ–°å‡½æ•°**:

```python
# ============ å¤š GPU è§‚æµ‹æŒ‡æ ‡ ============
class GPUQueueMetrics:
    """æ¯ä¸ª GPU é˜Ÿåˆ—çš„æ€§èƒ½æŒ‡æ ‡æ”¶é›†"""

    def __init__(self, gpu_id):
        self.gpu_id = gpu_id
        self.queue_lens = deque(maxlen=100)
        self.wait_times = deque(maxlen=100)
        self.exec_times = deque(maxlen=100)
        self.oom_count = 0
        self.total_tasks = 0
        self.success_tasks = 0
        self.failed_tasks = 0
        self.last_log_time = time.time()

    def record_task(self, queue_len, wait_ms, exec_ms, success=True, is_oom=False):
        """è®°å½•ä¸€æ¬¡ä»»åŠ¡æ‰§è¡Œ"""
        self.queue_lens.append(queue_len)
        self.wait_times.append(wait_ms)
        self.exec_times.append(exec_ms)

        if is_oom:
            self.oom_count += 1

        self.total_tasks += 1
        if success:
            self.success_tasks += 1
        else:
            self.failed_tasks += 1

        # æ¯ 10 ä¸ªä»»åŠ¡æˆ–æ¯ 60 ç§’è®°å½•ä¸€æ¬¡
        now = time.time()
        if self.total_tasks % 10 == 0 or (now - self.last_log_time) > 60:
            self.log_metrics()
            self.last_log_time = now

    def log_metrics(self):
        """è¾“å‡ºç»Ÿè®¡æŒ‡æ ‡"""
        if len(self.exec_times) == 0:
            return

        avg_queue = sum(self.queue_lens) / len(self.queue_lens)
        avg_wait = sum(self.wait_times) / len(self.wait_times)
        avg_exec = sum(self.exec_times) / len(self.exec_times)
        success_rate = (self.success_tasks / self.total_tasks * 100) if self.total_tasks > 0 else 0

        logging.info(
            f"ğŸ“Š [GPU {self.gpu_id}] "
            f"tasks={self.total_tasks}, "
            f"success={success_rate:.1f}%, "
            f"queue={avg_queue:.1f}, "
            f"wait={avg_wait:.0f}ms, "
            f"exec={avg_exec:.0f}ms, "
            f"oom={self.oom_count}"
        )


# ============ å¤š GPU Worker ============
def prompt_worker_gpu(gpu_id, queue, server_instance):
    """
    GPU ä¸“ç”¨ worker çº¿ç¨‹

    Args:
        gpu_id: GPU è®¾å¤‡ ID (0-3)
        queue: è¯¥ GPU çš„ä»»åŠ¡é˜Ÿåˆ—
        server_instance: æœåŠ¡å™¨å®ä¾‹
    """
    # è®¾ç½®çº¿ç¨‹è®¾å¤‡
    torch.cuda.set_device(gpu_id)
    logging.info(f"ğŸ”§ [GPU {gpu_id}] Worker started on cuda:{gpu_id}")

    # åˆ›å»ºä¸“å±æ‰§è¡Œå™¨
    cache_type = execution.CacheType.CLASSIC
    if args.cache_lru > 0:
        cache_type = execution.CacheType.LRU
    elif args.cache_ram > 0:
        cache_type = execution.CacheType.RAM_PRESSURE
    elif args.cache_none:
        cache_type = execution.CacheType.NONE

    executor = execution.PromptExecutor(
        server_instance,
        cache_type=cache_type,
        cache_args={"lru": args.cache_lru, "ram": args.cache_ram}
    )

    # åˆå§‹åŒ–æŒ‡æ ‡æ”¶é›†
    metrics = GPUQueueMetrics(gpu_id)

    # GC ç›¸å…³
    last_gc_collect = 0
    need_gc = False
    gc_collect_interval = 10.0

    # æ•…éšœæ¢å¤
    consecutive_failures = 0
    max_consecutive_failures = 3

    current_time = 0.0

    while True:
        timeout = 1000.0
        if need_gc:
            current_time = time.perf_counter()
            timeout = max(gc_collect_interval - (current_time - last_gc_collect), 0.0)

        # è®°å½•é˜Ÿåˆ—ç­‰å¾…å¼€å§‹æ—¶é—´
        queue_start_time = time.perf_counter()
        queue_item = queue.get(timeout=timeout)

        if queue_item is not None:
            item, item_id = queue_item

            # è®¡ç®—ç­‰å¾…æ—¶é—´
            wait_ms = (time.perf_counter() - queue_start_time) * 1000
            queue_len = len(queue.queue)

            # æ‰§è¡Œå¼€å§‹
            execution_start_time = time.perf_counter()
            prompt_id = item[1]
            server_instance.last_prompt_id = prompt_id

            is_oom = False
            success = False

            try:
                # åŒé‡è®¾å¤‡ä¿é™©
                with torch.cuda.device(gpu_id):
                    sensitive = item[5]
                    extra_data = item[3].copy()
                    for k in sensitive:
                        extra_data[k] = sensitive[k]

                    executor.execute(item[2], prompt_id, extra_data, item[4])

                need_gc = True
                consecutive_failures = 0
                success = executor.success

                # ä»»åŠ¡å®Œæˆ
                remove_sensitive = lambda prompt: prompt[:5] + prompt[6:]
                queue.task_done(
                    item_id,
                    executor.history_result,
                    status=execution.PromptQueue.ExecutionStatus(
                        status_str='success' if executor.success else 'error',
                        completed=executor.success,
                        messages=executor.status_messages
                    ),
                    process_item=remove_sensitive
                )

                if server_instance.client_id is not None:
                    server_instance.send_sync(
                        "executing",
                        {"node": None, "prompt_id": prompt_id},
                        server_instance.client_id
                    )

                current_time = time.perf_counter()
                execution_time = current_time - execution_start_time

                if execution_time > 600:
                    execution_time_str = time.strftime("%H:%M:%S", time.gmtime(execution_time))
                    logging.info(f"âœ… [GPU {gpu_id}] Prompt {prompt_id[:8]} executed in {execution_time_str}")
                else:
                    logging.info(f"âœ… [GPU {gpu_id}] Prompt {prompt_id[:8]} executed in {execution_time:.2f}s")

            except RuntimeError as e:
                # OOM å¤„ç†
                if "out of memory" in str(e).lower() or "OOM" in str(e):
                    is_oom = True
                    logging.error(f"ğŸ’¥ [GPU {gpu_id}] OOM for prompt {prompt_id[:8]}, clearing cache")

                    # æ¸…ç†å½“å‰è®¾å¤‡ç¼“å­˜
                    device = torch.device(f'cuda:{gpu_id}')
                    comfy.model_management.soft_empty_cache(device)
                    gc.collect()

                logging.error(f"âŒ [GPU {gpu_id}] Error executing prompt {prompt_id[:8]}: {e}")
                consecutive_failures += 1

                queue.task_done(
                    item_id,
                    {},
                    status=execution.PromptQueue.ExecutionStatus(
                        status_str='error',
                        completed=False,
                        messages=[str(e)]
                    )
                )

                # è¿ç»­å¤±è´¥è¿‡å¤šæ—¶é‡å»ºæ‰§è¡Œå™¨
                if consecutive_failures >= max_consecutive_failures:
                    logging.warning(f"âš ï¸  [GPU {gpu_id}] Too many failures ({consecutive_failures}), recreating executor")
                    executor = execution.PromptExecutor(
                        server_instance,
                        cache_type=cache_type,
                        cache_args={"lru": args.cache_lru, "ram": args.cache_ram}
                    )
                    consecutive_failures = 0

            except Exception as e:
                logging.error(f"âŒ [GPU {gpu_id}] Unexpected error: {e}")
                import traceback
                traceback.print_exc()

                consecutive_failures += 1

                queue.task_done(
                    item_id,
                    {},
                    status=execution.PromptQueue.ExecutionStatus(
                        status_str='error',
                        completed=False,
                        messages=[str(e)]
                    )
                )

            # è®°å½•æŒ‡æ ‡
            exec_ms = (time.perf_counter() - execution_start_time) * 1000
            metrics.record_task(queue_len, wait_ms, exec_ms, success=success, is_oom=is_oom)

        # GC å’Œå†…å­˜ç®¡ç†
        flags = queue.get_flags()
        free_memory_flag = flags.get("free_memory", False)

        if flags.get("unload_models", free_memory_flag):
            comfy.model_management.unload_all_models()
            need_gc = True
            last_gc_collect = 0

        if free_memory_flag:
            executor.reset()
            need_gc = True
            last_gc_collect = 0

        if need_gc:
            current_time = time.perf_counter()
            if (current_time - last_gc_collect) > gc_collect_interval:
                gc.collect()
                device = torch.device(f'cuda:{gpu_id}')
                comfy.model_management.soft_empty_cache(device)
                last_gc_collect = current_time
                need_gc = False
                hook_breaker_ac10a0.restore_functions()


# ============ GPU é¢„çƒ­ ============
def warmup_gpu(gpu_id):
    """
    é¢„çƒ­ GPUï¼šé¢„åˆ†é…æ˜¾å­˜ï¼Œé™ä½é¦–æ¬¡è¯·æ±‚å»¶è¿Ÿ

    Args:
        gpu_id: GPU è®¾å¤‡ ID
    """
    logging.info(f"ğŸ”¥ [GPU {gpu_id}] Starting warmup...")

    try:
        torch.cuda.set_device(gpu_id)

        with torch.cuda.device(gpu_id):
            # ç®€å•çš„ tensor åˆ†é…è§¦å‘ CUDA åˆå§‹åŒ–
            dummy = torch.zeros((1000, 1000), device=f'cuda:{gpu_id}', dtype=torch.float32)
            torch.cuda.synchronize()
            del dummy

        logging.info(f"âœ… [GPU {gpu_id}] Warmup completed")
    except Exception as e:
        logging.warning(f"âš ï¸  [GPU {gpu_id}] Warmup failed: {e}")


# ============ å…¼å®¹æ€§æ£€æŸ¥ ============
def check_custom_nodes_compatibility():
    """
    æ£€æŸ¥è‡ªå®šä¹‰èŠ‚ç‚¹ä¸­çš„è®¾å¤‡ç¡¬ç¼–ç é—®é¢˜
    æ‰«æ custom_nodes ç›®å½•ï¼ŒæŸ¥æ‰¾å¯èƒ½çš„ cuda:0 ç¡¬ç¼–ç 
    """
    if not ENABLE_MULTI_GPU:
        return

    logging.info("ğŸ” Checking custom nodes compatibility...")

    try:
        import re
        from pathlib import Path

        custom_nodes_paths = folder_paths.get_folder_paths("custom_nodes")
        if not custom_nodes_paths:
            return

        custom_nodes_path = Path(custom_nodes_paths[0])
        if not custom_nodes_path.exists():
            return

        issues = []

        # åŒ¹é…æ¨¡å¼ï¼šæ’é™¤å­—ç¬¦ä¸²å’Œæ³¨é‡Šä¸­çš„è¯¯æŠ¥
        # æŸ¥æ‰¾ cuda:0 ä½†ä¸åœ¨å¼•å·å†…çš„æƒ…å†µ
        pattern = re.compile(r'''(?<!['"(])cuda:0(?!['")])''')

        for py_file in custom_nodes_path.rglob("*.py"):
            if py_file.name.startswith('.'):
                continue

            try:
                content = py_file.read_text(encoding='utf-8', errors='ignore')

                for i, line in enumerate(content.split('\n'), 1):
                    # è·³è¿‡æ³¨é‡Šè¡Œ
                    stripped = line.strip()
                    if stripped.startswith('#'):
                        continue

                    if pattern.search(line):
                        rel_path = py_file.relative_to(custom_nodes_path)
                        issues.append(f"{rel_path}:{i}")

                        if len(issues) >= 20:  # æœ€å¤šæ”¶é›† 20 ä¸ª
                            break
            except Exception:
                pass

            if len(issues) >= 20:
                break

        if issues:
            logging.warning("=" * 70)
            logging.warning("âš ï¸  Found potential device hardcoding in custom nodes:")
            for issue in issues[:10]:
                logging.warning(f"  - {issue}")
            if len(issues) > 10:
                logging.warning(f"  ... and {len(issues) - 10} more")
            logging.warning("")
            logging.warning("These nodes may not work correctly with multi-GPU setup.")
            logging.warning("Please check if these are false positives or need fixing.")
            logging.warning("=" * 70)
        else:
            logging.info("âœ… No obvious device hardcoding detected")

    except Exception as e:
        logging.warning(f"âš ï¸  Compatibility check failed: {e}")
```

**ä¿®æ”¹ `start_comfyui()` å‡½æ•°ï¼ˆçº¦ line 303ï¼‰**:

æ‰¾åˆ°è¿™éƒ¨åˆ†ä»£ç ï¼š
```python
def start_comfyui(asyncio_loop=None):
    # ... å‰é¢é€»è¾‘ä¸å˜ï¼Œç›´åˆ°è¿™é‡Œï¼š

    prompt_server = server.PromptServer(asyncio_loop)

    # ========== ä¿®æ”¹å¼€å§‹ ==========
```

æ›¿æ¢ä¸ºï¼š
```python
    prompt_server = server.PromptServer(asyncio_loop)

    # ========== å¤š GPU æ¨¡å¼åˆå§‹åŒ– ==========
    if ENABLE_MULTI_GPU:
        # åˆ›å»ºå¤šä¸ªé˜Ÿåˆ—
        prompt_server.prompt_queues = [
            execution.PromptQueue(prompt_server) for _ in range(NUM_GPUS)
        ]
        # å…¼å®¹åŸæœ‰ä»£ç ï¼ˆæŒ‡å‘ GPU 0 çš„é˜Ÿåˆ—ï¼‰
        prompt_server.prompt_queue = prompt_server.prompt_queues[0]

        logging.info(f"ğŸ“‹ Created {NUM_GPUS} task queues for multi-GPU scheduling")
    else:
        # å• GPU æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        prompt_server.prompt_queue = execution.PromptQueue(prompt_server)
    # ========================================
```

æ‰¾åˆ°å¯åŠ¨ worker çš„ä»£ç ï¼ˆçº¦ line 339ï¼‰ï¼š
```python
    threading.Thread(target=prompt_worker, daemon=True, args=(prompt_server.prompt_queue, prompt_server,)).start()
```

æ›¿æ¢ä¸ºï¼š
```python
    # ========== å¯åŠ¨ Worker çº¿ç¨‹ ==========
    if ENABLE_MULTI_GPU:
        # å¤š GPU æ¨¡å¼ï¼šå¯åŠ¨å¤šä¸ª worker
        for gpu_id in range(NUM_GPUS):
            threading.Thread(
                target=prompt_worker_gpu,
                daemon=True,
                args=(gpu_id, prompt_server.prompt_queues[gpu_id], prompt_server),
                name=f"GPU{gpu_id}-Worker"
            ).start()
            logging.info(f"ğŸš€ Started worker thread for GPU {gpu_id}")

        # é¢„çƒ­æ‰€æœ‰ GPU
        for gpu_id in range(NUM_GPUS):
            warmup_gpu(gpu_id)

        # å…¼å®¹æ€§æ£€æŸ¥
        check_custom_nodes_compatibility()
    else:
        # å• GPU æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        threading.Thread(
            target=prompt_worker,
            daemon=True,
            args=(prompt_server.prompt_queue, prompt_server)
        ).start()
    # ======================================
```

#### 3.3 éªŒè¯æ”¹åŠ¨

```bash
# è¯­æ³•æ£€æŸ¥
python -m py_compile main.py

echo $?  # åº”è¯¥è¿”å› 0
```

---

### Step 4: ä¿®æ”¹ server.py

#### 4.1 ç›®æ ‡

- ä¿®æ”¹ `/prompt` æ¥å£ï¼Œæ”¯æŒ GPU è·¯ç”±åˆ†å‘
- ä»è¯·æ±‚å¤´ `X-TARGET-GPU` è¯»å–ç›®æ ‡ GPU
- å°†ä»»åŠ¡æäº¤åˆ°å¯¹åº”çš„é˜Ÿåˆ—

#### 4.2 å…·ä½“æ”¹åŠ¨

**ä½ç½®**: `server.py`

æ‰¾åˆ° `/prompt` è·¯ç”±å¤„ç†å‡½æ•°ï¼ˆæœç´¢ `@routes.post("/prompt")`ï¼Œçº¦ line 200-300 ä¹‹é—´ï¼‰

åœ¨å‡½æ•°å¼€å¤´æ·»åŠ  GPU è·¯ç”±é€»è¾‘ï¼š

```python
@routes.post("/prompt")
async def post_prompt(request):
    # ========== æ–°å¢ï¼šGPU è·¯ç”± ==========
    # è§£æç›®æ ‡ GPU ID
    gpu_id_str = request.headers.get('X-TARGET-GPU', '0')
    try:
        gpu_id = int(gpu_id_str)
        gpu_id = max(0, min(gpu_id, 3))  # é™åˆ¶åœ¨ 0-3
    except ValueError:
        gpu_id = 0
    # ==================================

    json_data = await request.json()
    json_data = DictX(json_data)

    # ... ç°æœ‰çš„éªŒè¯é€»è¾‘ ...

    # ========== ä¿®æ”¹ï¼šé€‰æ‹©é˜Ÿåˆ— ==========
    # åŸæœ‰ä»£ç ç±»ä¼¼ï¼š
    # prompt_id = str(uuid.uuid4())
    # self.prompt_queue.put((number, prompt_id, prompt, extra_data, outputs_to_execute, sensitive))

    # ä¿®æ”¹ä¸ºï¼š
    prompt_id = str(uuid.uuid4())

    # æ ¹æ®æ˜¯å¦å¯ç”¨å¤š GPU é€‰æ‹©é˜Ÿåˆ—
    if hasattr(self, 'prompt_queues') and len(self.prompt_queues) > gpu_id:
        target_queue = self.prompt_queues[gpu_id]
        logging.debug(f"Routing prompt {prompt_id[:8]} to GPU {gpu_id}")
    else:
        # å‘åå…¼å®¹ï¼šå•é˜Ÿåˆ—æ¨¡å¼
        target_queue = self.prompt_queue
        gpu_id = 0

    number = self.number
    target_queue.put((number, prompt_id, prompt, extra_data, outputs_to_execute, sensitive))
    self.number += 1
    # ==================================

    # è¿”å›å“åº”ï¼ˆæ–°å¢ gpu_id å­—æ®µï¼‰
    response = {
        "prompt_id": prompt_id,
        "number": number,
        "node_errors": valid[3]
    }

    # å¦‚æœæ˜¯å¤š GPU æ¨¡å¼ï¼Œè¿”å›åˆ†é…çš„ GPU ID
    if hasattr(self, 'prompt_queues'):
        response["gpu_id"] = gpu_id

    return web.json_response(response)
```

#### 4.3 éªŒè¯æ”¹åŠ¨

```bash
# è¯­æ³•æ£€æŸ¥
python -m py_compile server.py

echo $?  # åº”è¯¥è¿”å› 0
```

---

### Step 5: é…ç½® Nginx

#### 5.1 ç›®æ ‡

- é…ç½® 4 ä¸ªç«¯å£ï¼ˆ8181-8184ï¼‰å¯¹åº” 4 å¼  GPU
- æ¯ä¸ªç«¯å£æ·»åŠ è¯·æ±‚å¤´ `X-TARGET-GPU`
- é…ç½®é™æµé˜²æ­¢è¿‡è½½

#### 5.2 åˆ›å»º Nginx é…ç½®

**åˆ›å»ºæ–‡ä»¶**: `/etc/nginx/sites-available/comfyui-multi-gpu` æˆ– `./nginx.conf`

```nginx
# ComfyUI å¤š GPU è´Ÿè½½å‡è¡¡é…ç½®

# é™æµé…ç½®ï¼šä¸ºæ¯ä¸ª GPU å®šä¹‰ç‹¬ç«‹çš„é™æµåŒº
limit_req_zone $binary_remote_addr zone=gpu0_limit:10m rate=10r/s;
limit_req_zone $binary_remote_addr zone=gpu1_limit:10m rate=10r/s;
limit_req_zone $binary_remote_addr zone=gpu2_limit:10m rate=10r/s;
limit_req_zone $binary_remote_addr zone=gpu3_limit:10m rate=10r/s;

# ComfyUI åç«¯
upstream comfyui_backend {
    server 127.0.0.1:8188;
    keepalive 32;
}

# GPU 0 å…¥å£ - ç«¯å£ 8181
server {
    listen 8181;
    server_name _;

    client_max_body_size 100M;

    location / {
        # é™æµï¼šæ¯ç§’ 10 ä¸ªè¯·æ±‚ï¼Œçªå‘ 20 ä¸ª
        limit_req zone=gpu0_limit burst=20 nodelay;

        # è®¾ç½®ç›®æ ‡ GPU
        proxy_set_header X-TARGET-GPU 0;

        # æ ‡å‡†ä»£ç†é…ç½®
        proxy_pass http://comfyui_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # è¶…æ—¶è®¾ç½®ï¼ˆç”Ÿå›¾å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ï¼‰
        proxy_read_timeout 600s;
        proxy_connect_timeout 60s;
        proxy_send_timeout 600s;
    }
}

# GPU 1 å…¥å£ - ç«¯å£ 8182
server {
    listen 8182;
    server_name _;

    client_max_body_size 100M;

    location / {
        limit_req zone=gpu1_limit burst=20 nodelay;
        proxy_set_header X-TARGET-GPU 1;

        proxy_pass http://comfyui_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        proxy_read_timeout 600s;
        proxy_connect_timeout 60s;
        proxy_send_timeout 600s;
    }
}

# GPU 2 å…¥å£ - ç«¯å£ 8183
server {
    listen 8183;
    server_name _;

    client_max_body_size 100M;

    location / {
        limit_req zone=gpu2_limit burst=20 nodelay;
        proxy_set_header X-TARGET-GPU 2;

        proxy_pass http://comfyui_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        proxy_read_timeout 600s;
        proxy_connect_timeout 60s;
        proxy_send_timeout 600s;
    }
}

# GPU 3 å…¥å£ - ç«¯å£ 8184
server {
    listen 8184;
    server_name _;

    client_max_body_size 100M;

    location / {
        limit_req zone=gpu3_limit burst=20 nodelay;
        proxy_set_header X-TARGET-GPU 3;

        proxy_pass http://comfyui_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        proxy_read_timeout 600s;
        proxy_connect_timeout 60s;
        proxy_send_timeout 600s;
    }
}
```

#### 5.3 å¯ç”¨é…ç½®

```bash
# å¦‚æœä½¿ç”¨ç³»ç»Ÿ Nginx
sudo ln -s /etc/nginx/sites-available/comfyui-multi-gpu /etc/nginx/sites-enabled/
sudo nginx -t  # æµ‹è¯•é…ç½®
sudo systemctl reload nginx

# å¦‚æœä½¿ç”¨æœ¬åœ° Nginx
nginx -c /path/to/nginx.conf -t  # æµ‹è¯•
nginx -c /path/to/nginx.conf     # å¯åŠ¨
```

#### 5.4 æ›¿ä»£æ–¹æ¡ˆï¼šä½¿ç”¨è·¯å¾„å‰ç¼€

å¦‚æœä¸æƒ³å¼€ 4 ä¸ªç«¯å£ï¼Œå¯ä»¥ç”¨è·¯å¾„å‰ç¼€ï¼š

```nginx
server {
    listen 8180;
    server_name _;

    client_max_body_size 100M;

    location /gpu0/ {
        limit_req zone=gpu0_limit burst=20 nodelay;
        proxy_set_header X-TARGET-GPU 0;
        rewrite ^/gpu0/(.*) /$1 break;
        proxy_pass http://comfyui_backend;
        # ... å…¶ä»–é…ç½®åŒä¸Š
    }

    location /gpu1/ {
        limit_req zone=gpu1_limit burst=20 nodelay;
        proxy_set_header X-TARGET-GPU 1;
        rewrite ^/gpu1/(.*) /$1 break;
        proxy_pass http://comfyui_backend;
        # ... å…¶ä»–é…ç½®åŒä¸Š
    }

    # ç±»ä¼¼é…ç½® /gpu2/, /gpu3/
}
```

---

### Step 6: æµ‹è¯•éªŒè¯

#### 6.1 å¯åŠ¨ ComfyUIï¼ˆå¤š GPU æ¨¡å¼ï¼‰

åˆ›å»ºå¯åŠ¨è„šæœ¬ `start_multi_gpu.sh`:

```bash
#!/bin/bash

# è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0,1,2,3
export COMFY_MULTI_GPU_SCHED=1
export COMFY_NUM_GPUS=4

# é™åˆ¶ CPU çº¿ç¨‹æ•°ï¼ˆå¯é€‰ï¼‰
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4

# å¯åŠ¨ ComfyUI
python main.py --listen 0.0.0.0 --port 8188
```

```bash
chmod +x start_multi_gpu.sh
./start_multi_gpu.sh
```

**æ£€æŸ¥å¯åŠ¨æ—¥å¿—**ï¼Œåº”è¯¥çœ‹åˆ°ï¼š

```
âœ… Multi-GPU scheduling ENABLED
ğŸ“‹ Created 4 task queues for multi-GPU scheduling
ğŸš€ Started worker thread for GPU 0
ğŸš€ Started worker thread for GPU 1
ğŸš€ Started worker thread for GPU 2
ğŸš€ Started worker thread for GPU 3
ğŸ”¥ [GPU 0] Starting warmup...
âœ… [GPU 0] Warmup completed
ğŸ”¥ [GPU 1] Starting warmup...
âœ… [GPU 1] Warmup completed
...
ğŸ” Checking custom nodes compatibility...
âœ… No obvious device hardcoding detected
```

#### 6.2 åŸºç¡€åŠŸèƒ½æµ‹è¯•

**æµ‹è¯• 1: å•ä¸ªè¯·æ±‚åˆ°ä¸åŒ GPU**

```bash
# æµ‹è¯• GPU 0ï¼ˆç«¯å£ 8181ï¼‰
curl -X POST http://localhost:8181/prompt \
  -H "Content-Type: application/json" \
  -d '{"prompt": {...}}'  # ä½ çš„ workflow JSON

# æµ‹è¯• GPU 1ï¼ˆç«¯å£ 8182ï¼‰
curl -X POST http://localhost:8182/prompt \
  -H "Content-Type: application/json" \
  -d '{"prompt": {...}}'
```

**æ£€æŸ¥æ—¥å¿—**ï¼Œåº”è¯¥çœ‹åˆ°ï¼š
```
Routing prompt xxxxx to GPU 0
âœ… [GPU 0] Prompt xxxxx executed in 5.23s

Routing prompt yyyyy to GPU 1
âœ… [GPU 1] Prompt yyyyy executed in 5.18s
```

**æµ‹è¯• 2: å¹¶å‘è¯·æ±‚**

```bash
# åŒæ—¶å‘ 4 ä¸ªç«¯å£å‘é€è¯·æ±‚
for i in {0..3}; do
  port=$((8181 + i))
  curl -X POST http://localhost:$port/prompt \
    -H "Content-Type: application/json" \
    -d '{"prompt": {...}}' &
done
wait
```

**æ£€æŸ¥ GPU ä½¿ç”¨**ï¼š
```bash
watch -n 1 nvidia-smi
```

åº”è¯¥çœ‹åˆ° 4 å¼  GPU åŒæ—¶æœ‰è´Ÿè½½ã€‚

#### 6.3 RAM å…±äº«éªŒè¯

```bash
# è®°å½•å¯åŠ¨æ—¶çš„ RSS
ps aux | grep "python main.py"
# è®°ä½ RSS åˆ—çš„å€¼ï¼ˆä¾‹å¦‚ 8.5GBï¼‰

# å‘é€ 10 ä¸ªè¯·æ±‚ï¼ˆç›¸åŒæ¨¡å‹ï¼‰
for i in {1..10}; do
  curl -X POST http://localhost:8181/prompt -H "Content-Type: application/json" -d '{"prompt": {...}}'
done

# å†æ¬¡æ£€æŸ¥ RSS
ps aux | grep "python main.py"
# RSS åº”è¯¥æ²¡æœ‰æ˜æ˜¾å¢é•¿ï¼ˆå¢é•¿ <10%ï¼‰
```

#### 6.4 å›æ»šæµ‹è¯•

```bash
# åœæ­¢å¤š GPU æ¨¡å¼
# Ctrl+C

# å¯åŠ¨å• GPU æ¨¡å¼
export COMFY_MULTI_GPU_SCHED=0
python main.py --listen 0.0.0.0 --port 8188
```

**æ£€æŸ¥æ—¥å¿—**ï¼Œåº”è¯¥çœ‹åˆ°ï¼š
```
â„¹ï¸  Multi-GPU scheduling DISABLED (using default mode)
â„¹ï¸  Single-GPU mode (default)
```

å‘é€è¯·æ±‚åº”è¯¥æ­£å¸¸å·¥ä½œï¼ˆèµ°åŸæœ‰é€»è¾‘ï¼‰ã€‚

---

## 5. éªŒæ”¶æ ‡å‡†

### 5.1 åŠŸèƒ½éªŒæ”¶

| ç¼–å· | éªŒæ”¶é¡¹ | éªŒè¯æ–¹æ³• | é¢„æœŸç»“æœ |
|------|-------|---------|---------|
| 1 | å•è¿›ç¨‹è¿è¡Œ | `ps aux \| grep python` | åªæœ‰ 1 ä¸ª Python è¿›ç¨‹ |
| 2 | 4 GPU å¹¶è¡Œ | åŒæ—¶å‘ 4 ä¸ªè¯·æ±‚ + `nvidia-smi` | 4 å¡åŒæ—¶æœ‰è´Ÿè½½ |
| 3 | å›ºå®šè·¯ç”± | å¤šæ¬¡è¯·æ±‚åŒä¸€ç«¯å£ + æ—¥å¿— | æ€»æ˜¯è·¯ç”±åˆ°ç›¸åŒ GPU |
| 4 | RAM å…±äº« | é‡å¤æäº¤ç›¸åŒæ¨¡å‹ + `ps aux` | RSS å¢é•¿ <10% |
| 5 | æ¨¡æ¿å…¼å®¹ | ä½¿ç”¨ç°æœ‰ workflow æµ‹è¯• | æ— éœ€ä¿®æ”¹å³å¯è¿è¡Œ |
| 6 | å›æ»šåŠŸèƒ½ | `COMFY_MULTI_GPU_SCHED=0` å¯åŠ¨ | æ¢å¤å• GPU æ¨¡å¼ |

### 5.2 æ€§èƒ½éªŒæ”¶

| æŒ‡æ ‡ | ç›®æ ‡ | éªŒè¯æ–¹æ³• |
|------|------|---------|
| **ååé‡** | æå‡ 3.5-4x | å‹æµ‹å¯¹æ¯”å•/å¤š GPU |
| **é¦–æ¬¡å»¶è¿Ÿ** | <2sï¼ˆé¢„çƒ­åï¼‰ | æµ‹é‡ç¬¬ä¸€ä¸ªè¯·æ±‚å“åº”æ—¶é—´ |
| **å¹¶å‘ç¨³å®šæ€§** | æ— å´©æºƒ | æŒç»­è¿è¡Œ 1 å°æ—¶ |
| **OOM æ¢å¤** | è‡ªåŠ¨æ¸…ç¼“å­˜ | æ•…æ„è§¦å‘ OOMï¼Œæ£€æŸ¥æ˜¯å¦æ¢å¤ |

### 5.3 æ—¥å¿—éªŒæ”¶

å¯åŠ¨ååº”çœ‹åˆ°ï¼š
```
âœ… Multi-GPU scheduling ENABLED
ğŸ“‹ Created 4 task queues
ğŸš€ Started worker thread for GPU 0/1/2/3
ğŸ”¥ [GPU x] Starting warmup...
âœ… [GPU x] Warmup completed
ğŸ” Checking custom nodes compatibility...
```

è¿è¡Œæ—¶åº”å®šæœŸçœ‹åˆ°ï¼š
```
ğŸ“Š [GPU 0] tasks=10, success=100.0%, queue=0.5, wait=12ms, exec=5234ms, oom=0
```

### 5.4 å…¼å®¹æ€§éªŒæ”¶

- âœ… ç°æœ‰å‰ç«¯æ— éœ€ä¿®æ”¹
- âœ… ç°æœ‰ workflow JSON æ— éœ€ä¿®æ”¹
- âœ… ç°æœ‰è‡ªå®šä¹‰èŠ‚ç‚¹æ­£å¸¸å·¥ä½œï¼ˆé™¤éæœ‰ç¡¬ç¼–ç ï¼‰

---

## 6. å›æ»šæ–¹æ¡ˆ

### 6.1 å¿«é€Ÿå›æ»šï¼ˆç¯å¢ƒå˜é‡ï¼‰

```bash
# æ–¹æ³• 1ï¼šä¿®æ”¹å¯åŠ¨è„šæœ¬
export COMFY_MULTI_GPU_SCHED=0
python main.py --listen 0.0.0.0 --port 8188

# æ–¹æ³• 2ï¼šä¿®æ”¹ systemd service
sudo systemctl edit comfyui
# æ·»åŠ ï¼š
# [Service]
# Environment="COMFY_MULTI_GPU_SCHED=0"
sudo systemctl restart comfyui
```

### 6.2 ä»£ç å›æ»š

```bash
# ä»å¤‡ä»½æ¢å¤
BACKUP_DIR="backups/20251105_123456"  # ä½ çš„å¤‡ä»½ç›®å½•

cp "$BACKUP_DIR/model_management.py" comfy/
cp "$BACKUP_DIR/main.py" .
cp "$BACKUP_DIR/server.py" .

# é‡å¯æœåŠ¡
python main.py --listen 0.0.0.0 --port 8188
```

### 6.3 Git å›æ»š

```bash
git checkout main
git branch -D feature/multi-gpu-sched
```

---

## 7. æ•…éšœæ’æŸ¥

### 7.1 å¸¸è§é—®é¢˜

#### é—®é¢˜ 1: å¯åŠ¨æ—¶æŠ¥é”™ `NameError: name '_get_current_loaded_models' is not defined`

**åŸå› **: model_management.py æ”¹åŠ¨ä¸å®Œæ•´

**è§£å†³**:
```bash
# æ£€æŸ¥æ˜¯å¦æ·»åŠ äº†å‡½æ•°å®šä¹‰
grep "_get_current_loaded_models" comfy/model_management.py

# å¦‚æœæ²¡æœ‰ï¼Œé‡æ–°æ·»åŠ ï¼ˆå‚è€ƒ Step 2ï¼‰
```

#### é—®é¢˜ 2: æ‰€æœ‰è¯·æ±‚éƒ½è·¯ç”±åˆ° GPU 0

**åŸå› **: Nginx æœªæ­£ç¡®è®¾ç½® Header

**è§£å†³**:
```bash
# æ£€æŸ¥ Nginx é…ç½®
nginx -T | grep X-TARGET-GPU

# åº”è¯¥çœ‹åˆ°ï¼š
# proxy_set_header X-TARGET-GPU 0;
# proxy_set_header X-TARGET-GPU 1;
# ...

# é‡æ–°åŠ è½½ Nginx
sudo nginx -s reload
```

#### é—®é¢˜ 3: GPU 1/2/3 æ²¡æœ‰è´Ÿè½½

**åŸå› **: Worker çº¿ç¨‹æœªå¯åŠ¨æˆ–å´©æºƒ

**è§£å†³**:
```bash
# æ£€æŸ¥æ—¥å¿—ä¸­æ˜¯å¦æœ‰ Worker å¯åŠ¨ä¿¡æ¯
grep "Worker started" comfyui.log

# æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å †æ ˆ
grep "Traceback" comfyui.log

# æ£€æŸ¥çº¿ç¨‹çŠ¶æ€
python -c "
import os
os.environ['COMFY_MULTI_GPU_SCHED'] = '1'
exec(open('main.py').read())
" | grep "GPU.*Worker"
```

#### é—®é¢˜ 4: RAM æŒç»­å¢é•¿

**åŸå› **: æ¨¡å‹ç¼“å­˜æœªæ­£ç¡®æ¸…ç†

**è§£å†³**:
```bash
# æ£€æŸ¥ free_memory æ˜¯å¦è¢«è°ƒç”¨
grep "Unloading" comfyui.log

# æ‰‹åŠ¨è§¦å‘æ¸…ç†
curl -X POST http://localhost:8188/free -d '{"unload_models": true}'

# æ£€æŸ¥ç¼“å­˜åˆ†åŒºæ˜¯å¦ç”Ÿæ•ˆ
python -c "
import comfy.model_management as mm
print('Using device cache:', mm._use_device_cache)
"
```

#### é—®é¢˜ 5: OOM åå´©æºƒ

**åŸå› **: OOM å¤„ç†æœªç”Ÿæ•ˆ

**æ£€æŸ¥æ—¥å¿—**:
```bash
grep "OOM" comfyui.log
# åº”è¯¥çœ‹åˆ°ï¼š
# ğŸ’¥ [GPU x] OOM for prompt..., clearing cache
```

**å¦‚æœæ²¡æœ‰**ï¼Œæ£€æŸ¥ä»£ç ä¸­æ˜¯å¦æ­£ç¡®æ•è·äº† `RuntimeError`ã€‚

### 7.2 è°ƒè¯•å·¥å…·

#### æŸ¥çœ‹é˜Ÿåˆ—çŠ¶æ€

```python
# æ·»åŠ è°ƒè¯•æ¥å£åˆ° server.py
@routes.get("/debug/queues")
async def get_queue_status(request):
    if not hasattr(self, 'prompt_queues'):
        return web.json_response({"mode": "single-gpu"})

    status = []
    for i, q in enumerate(self.prompt_queues):
        running, queued = q.get_current_queue_volatile()
        status.append({
            "gpu_id": i,
            "queued": len(queued),
            "running": len(running)
        })

    return web.json_response({"queues": status})
```

```bash
# æŸ¥è¯¢
curl http://localhost:8188/debug/queues
```

#### æŸ¥çœ‹æ¨¡å‹ç¼“å­˜

```python
# åœ¨ Python REPL ä¸­
import comfy.model_management as mm

if mm._use_device_cache:
    for device_id, cache in mm._current_loaded_models_by_device.items():
        print(f"GPU {device_id}: {len(cache)} models")
        for m in cache:
            print(f"  - {m.model.model.__class__.__name__}")
```

---

## 8. æ€§èƒ½è°ƒä¼˜

### 8.1 è°ƒæ•´æ¯å¡å¹¶å‘

```bash
# å¦‚æœæ˜¾å­˜è¶³å¤Ÿï¼ˆ>24GBï¼‰ï¼Œå¯ä»¥å°è¯•æ¯å¡å¹¶å‘ 2
# ä¿®æ”¹ prompt_worker_gpu åˆ›å»º 2 ä¸ªå®ä¾‹

# æˆ–è€…ä½¿ç”¨é˜Ÿåˆ—ä¼˜å…ˆçº§ï¼ˆæœªå®ç°ï¼Œå¯æ‰©å±•ï¼‰
```

### 8.2 è°ƒæ•´ GC é—´éš”

```python
# main.py prompt_worker_gpu å‡½æ•°ä¸­
gc_collect_interval = 10.0  # é»˜è®¤ 10 ç§’

# å¦‚æœå†…å­˜å‹åŠ›å¤§ï¼Œæ”¹ä¸º 5.0
# å¦‚æœè¿½æ±‚æ€§èƒ½ï¼Œæ”¹ä¸º 15.0
```

### 8.3 è°ƒæ•´é™æµå‚æ•°

```nginx
# nginx.conf
limit_req zone=gpu0_limit burst=20 nodelay;

# å¦‚æœé˜Ÿåˆ—å †ç§¯ï¼Œé™ä½ burst
# burst=10

# å¦‚æœæ‹’ç»ç‡é«˜ï¼Œæé«˜ rate
# rate=15r/s
```

### 8.4 ç›‘æ§æŒ‡æ ‡

```bash
# å®æ—¶ç›‘æ§ GPU
watch -n 1 nvidia-smi

# å®æ—¶ç›‘æ§æ—¥å¿—
tail -f comfyui.log | grep "ğŸ“Š"

# ç»Ÿè®¡ OOM æ¬¡æ•°
grep "OOM" comfyui.log | wc -l

# ç»Ÿè®¡æˆåŠŸç‡
grep "âœ….*executed" comfyui.log | wc -l
grep "âŒ.*Error" comfyui.log | wc -l
```

---

## é™„å½• A: å®Œæ•´å¯åŠ¨è„šæœ¬ç¤ºä¾‹

```bash
#!/bin/bash
# start_multi_gpu.sh

set -e

echo "ğŸš€ Starting ComfyUI Multi-GPU Mode"

# ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0,1,2,3
export COMFY_MULTI_GPU_SCHED=1
export COMFY_NUM_GPUS=4

# CPU çº¿ç¨‹é™åˆ¶
export OMP_NUM_THREADS=4
export MKL_NUM_THREADS=4

# å¯é€‰ï¼šå¯ç”¨ CUDA ä¼˜åŒ–
export CUDA_LAUNCH_BLOCKING=0

# æ—¥å¿—
LOG_DIR="logs"
mkdir -p "$LOG_DIR"
LOG_FILE="$LOG_DIR/comfyui_$(date +%Y%m%d_%H%M%S).log"

# å¯åŠ¨
echo "ğŸ“ Log file: $LOG_FILE"
python main.py \
    --listen 0.0.0.0 \
    --port 8188 \
    2>&1 | tee "$LOG_FILE"
```

---

## é™„å½• B: å‹æµ‹è„šæœ¬ç¤ºä¾‹

```bash
#!/bin/bash
# benchmark.sh

# æµ‹è¯•å• GPU ååé‡
echo "Testing single-GPU mode..."
export COMFY_MULTI_GPU_SCHED=0
python main.py --port 8188 &
PID=$!
sleep 10

# å‘é€ 20 ä¸ªè¯·æ±‚
time for i in {1..20}; do
  curl -s -X POST http://localhost:8188/prompt \
    -H "Content-Type: application/json" \
    -d @workflow.json > /dev/null
done

kill $PID

# æµ‹è¯•å¤š GPU ååé‡
echo "Testing multi-GPU mode..."
export COMFY_MULTI_GPU_SCHED=1
python main.py --port 8188 &
PID=$!
sleep 10

# å¹¶å‘å‘é€ 20 ä¸ªè¯·æ±‚ï¼ˆæ¯ä¸ª GPU 5 ä¸ªï¼‰
time for i in {0..3}; do
  port=$((8181 + i))
  for j in {1..5}; do
    curl -s -X POST http://localhost:$port/prompt \
      -H "Content-Type: application/json" \
      -d @workflow.json > /dev/null &
  done
done
wait

kill $PID
```

---

## é™„å½• C: Systemd æœåŠ¡é…ç½®

```ini
# /etc/systemd/system/comfyui-multi-gpu.service

[Unit]
Description=ComfyUI Multi-GPU Service
After=network.target

[Service]
Type=simple
User=comfyui
WorkingDirectory=/home/comfyui/ComfyUI
Environment="CUDA_VISIBLE_DEVICES=0,1,2,3"
Environment="COMFY_MULTI_GPU_SCHED=1"
Environment="COMFY_NUM_GPUS=4"
Environment="OMP_NUM_THREADS=4"
ExecStart=/usr/bin/python3 main.py --listen 0.0.0.0 --port 8188
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

```bash
# å¯ç”¨æœåŠ¡
sudo systemctl daemon-reload
sudo systemctl enable comfyui-multi-gpu
sudo systemctl start comfyui-multi-gpu
sudo systemctl status comfyui-multi-gpu
```

---

## æ€»ç»“

æœ¬æ–‡æ¡£æä¾›äº† ComfyUI å¤š GPU å¹¶è¡Œæ”¹é€ çš„å®Œæ•´å®æ–½æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ï¼š

- âœ… è¯¦ç»†çš„æ”¹åŠ¨æ­¥éª¤ï¼ˆStep 1-6ï¼‰
- âœ… å®Œæ•´çš„éªŒæ”¶æ ‡å‡†
- âœ… å¯é çš„å›æ»šæ–¹æ¡ˆ
- âœ… å…¨é¢çš„æ•…éšœæ’æŸ¥æŒ‡å—
- âœ… æ€§èƒ½è°ƒä¼˜å»ºè®®

æŒ‰ç…§æœ¬æ–‡æ¡£é€æ­¥æ‰§è¡Œï¼Œé¢„è®¡ 2-4 å°æ—¶å¯å®Œæˆæ”¹é€ ï¼Œå®ç°ï¼š
- å•è¿›ç¨‹å…±äº« RAM
- 4 GPU çœŸå¹¶è¡Œ
- ååé‡æå‡ 3.5-4x
- é›¶é£é™©å¯å›æ»š

**ä¸‹ä¸€æ­¥**: ä» [Step 1: å¤‡ä»½ä¸å‡†å¤‡](#step-1-å¤‡ä»½ä¸å‡†å¤‡) å¼€å§‹æ‰§è¡Œã€‚
